{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMI9LCtwrelU8jFg+lTewoQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SinghhSarvesh/NEXT-GEN-AI/blob/main/NLP_Based_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Zh5f4Kh4hJBV"
      },
      "outputs": [],
      "source": [
        "# tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "# stopwords list\n",
        "from nltk.corpus import stopwords\n",
        "# stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "#lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [(\"hello\",\"greet\"),\n",
        "                 (\"hello there\",\"greet\"),\n",
        "                 (\"hey\",\"greet\"),\n",
        "                 (\"good morning\",\"greet\"),\n",
        "                 (\"what's the weather today\",\"weather\"),\n",
        "                 (\"what's the temperature\",\"weather\"),\n",
        "                 (\"is it raining\",\"weather\"),\n",
        "                 (\"open google\",\"open_web\"),\n",
        "                 (\"open facebook\",\"open_web\"),\n",
        "                 (\"open youtube\",\"open_web\"),\n",
        "                 (\"bye\",\"exit\"),\n",
        "                 (\"googdbye\",\"exit\"),\n",
        "                 (\"exit\",\"exit\")\n",
        "]"
      ],
      "metadata": {
        "id": "W_0L8sGhhMoF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "for text, intent in training_data:\n",
        "  sentences.append(text)\n",
        "  labels.append(intent)"
      ],
      "metadata": {
        "id": "uKPC5CpChdx8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def preprocess_text(documents):\n",
        "    english_stopwords = stopwords.words(\"english\")\n",
        "    punctuations = string.punctuation\n",
        "    cleaned_documents = []\n",
        "    for doc in documents:\n",
        "        # Step-1 : Lowercase\n",
        "        raw_text = doc.lower()\n",
        "        print(\"After lowercase: \",raw_text)\n",
        "\n",
        "        tokens = word_tokenize(raw_text)\n",
        "        print(\"Tokens:\",tokens)\n",
        "\n",
        "        filtered_tokens = []\n",
        "        for word in tokens:\n",
        "            if word not in english_stopwords:\n",
        "                filtered_tokens.append(word)\n",
        "\n",
        "        print(\"Filtered Tokens :\",filtered_tokens)\n",
        "\n",
        "        clean_tokens = [word for word in filtered_tokens if word not in punctuations]\n",
        "        print(\"After removing punctuations:\",clean_tokens)\n",
        "\n",
        "        lemmatized_words = []\n",
        "        wnet = WordNetLemmatizer()\n",
        "        for word in clean_tokens:\n",
        "            lemmatized_words.append(wnet.lemmatize(word,\"v\"))\n",
        "\n",
        "        print(\"After Lemmatization :\",lemmatized_words)\n",
        "\n",
        "        final_tokens = []\n",
        "        for word in lemmatized_words:\n",
        "            if word.isalpha():\n",
        "                final_tokens.append(word)\n",
        "\n",
        "        print(\"Final Tokens:\",final_tokens)\n",
        "\n",
        "        cleaned_text = \" \".join(final_tokens)\n",
        "        print(\"Cleaned Text:\",cleaned_text)\n",
        "\n",
        "        cleaned_documents.append(cleaned_text)\n",
        "        print(\"=\"*50)\n",
        "    return cleaned_documents\n",
        "\n",
        "cleaned_sentences = preprocess_text(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u48yzwYsiJih",
        "outputId": "75bf0389-72b6-4d49-f63b-8b4b9054939c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After lowercase:  hello\n",
            "Tokens: ['hello']\n",
            "Filtered Tokens : ['hello']\n",
            "After removing punctuations: ['hello']\n",
            "After Lemmatization : ['hello']\n",
            "Final Tokens: ['hello']\n",
            "Cleaned Text: hello\n",
            "==================================================\n",
            "After lowercase:  hello there\n",
            "Tokens: ['hello', 'there']\n",
            "Filtered Tokens : ['hello']\n",
            "After removing punctuations: ['hello']\n",
            "After Lemmatization : ['hello']\n",
            "Final Tokens: ['hello']\n",
            "Cleaned Text: hello\n",
            "==================================================\n",
            "After lowercase:  hey\n",
            "Tokens: ['hey']\n",
            "Filtered Tokens : ['hey']\n",
            "After removing punctuations: ['hey']\n",
            "After Lemmatization : ['hey']\n",
            "Final Tokens: ['hey']\n",
            "Cleaned Text: hey\n",
            "==================================================\n",
            "After lowercase:  good morning\n",
            "Tokens: ['good', 'morning']\n",
            "Filtered Tokens : ['good', 'morning']\n",
            "After removing punctuations: ['good', 'morning']\n",
            "After Lemmatization : ['good', 'morning']\n",
            "Final Tokens: ['good', 'morning']\n",
            "Cleaned Text: good morning\n",
            "==================================================\n",
            "After lowercase:  what's the weather today\n",
            "Tokens: ['what', \"'s\", 'the', 'weather', 'today']\n",
            "Filtered Tokens : [\"'s\", 'weather', 'today']\n",
            "After removing punctuations: [\"'s\", 'weather', 'today']\n",
            "After Lemmatization : [\"'s\", 'weather', 'today']\n",
            "Final Tokens: ['weather', 'today']\n",
            "Cleaned Text: weather today\n",
            "==================================================\n",
            "After lowercase:  what's the temperature\n",
            "Tokens: ['what', \"'s\", 'the', 'temperature']\n",
            "Filtered Tokens : [\"'s\", 'temperature']\n",
            "After removing punctuations: [\"'s\", 'temperature']\n",
            "After Lemmatization : [\"'s\", 'temperature']\n",
            "Final Tokens: ['temperature']\n",
            "Cleaned Text: temperature\n",
            "==================================================\n",
            "After lowercase:  is it raining\n",
            "Tokens: ['is', 'it', 'raining']\n",
            "Filtered Tokens : ['raining']\n",
            "After removing punctuations: ['raining']\n",
            "After Lemmatization : ['rain']\n",
            "Final Tokens: ['rain']\n",
            "Cleaned Text: rain\n",
            "==================================================\n",
            "After lowercase:  open google\n",
            "Tokens: ['open', 'google']\n",
            "Filtered Tokens : ['open', 'google']\n",
            "After removing punctuations: ['open', 'google']\n",
            "After Lemmatization : ['open', 'google']\n",
            "Final Tokens: ['open', 'google']\n",
            "Cleaned Text: open google\n",
            "==================================================\n",
            "After lowercase:  open facebook\n",
            "Tokens: ['open', 'facebook']\n",
            "Filtered Tokens : ['open', 'facebook']\n",
            "After removing punctuations: ['open', 'facebook']\n",
            "After Lemmatization : ['open', 'facebook']\n",
            "Final Tokens: ['open', 'facebook']\n",
            "Cleaned Text: open facebook\n",
            "==================================================\n",
            "After lowercase:  open youtube\n",
            "Tokens: ['open', 'youtube']\n",
            "Filtered Tokens : ['open', 'youtube']\n",
            "After removing punctuations: ['open', 'youtube']\n",
            "After Lemmatization : ['open', 'youtube']\n",
            "Final Tokens: ['open', 'youtube']\n",
            "Cleaned Text: open youtube\n",
            "==================================================\n",
            "After lowercase:  bye\n",
            "Tokens: ['bye']\n",
            "Filtered Tokens : ['bye']\n",
            "After removing punctuations: ['bye']\n",
            "After Lemmatization : ['bye']\n",
            "Final Tokens: ['bye']\n",
            "Cleaned Text: bye\n",
            "==================================================\n",
            "After lowercase:  googdbye\n",
            "Tokens: ['googdbye']\n",
            "Filtered Tokens : ['googdbye']\n",
            "After removing punctuations: ['googdbye']\n",
            "After Lemmatization : ['googdbye']\n",
            "Final Tokens: ['googdbye']\n",
            "Cleaned Text: googdbye\n",
            "==================================================\n",
            "After lowercase:  exit\n",
            "Tokens: ['exit']\n",
            "Filtered Tokens : ['exit']\n",
            "After removing punctuations: ['exit']\n",
            "After Lemmatization : ['exit']\n",
            "Final Tokens: ['exit']\n",
            "Cleaned Text: exit\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoYk_murjkxF",
        "outputId": "e9544f39-baf8-4391-ee57-440e5dbd6144"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'hello there', 'hey', 'good morning', \"what's the weather today\"]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data = preprocess_text(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZsKGb75kx5V",
        "outputId": "885c3916-b02f-4b3f-b74a-8c43fbc66826"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After lowercase:  hello\n",
            "Tokens: ['hello']\n",
            "Filtered Tokens : ['hello']\n",
            "After removing punctuations: ['hello']\n",
            "After Lemmatization : ['hello']\n",
            "Final Tokens: ['hello']\n",
            "Cleaned Text: hello\n",
            "==================================================\n",
            "After lowercase:  hello there\n",
            "Tokens: ['hello', 'there']\n",
            "Filtered Tokens : ['hello']\n",
            "After removing punctuations: ['hello']\n",
            "After Lemmatization : ['hello']\n",
            "Final Tokens: ['hello']\n",
            "Cleaned Text: hello\n",
            "==================================================\n",
            "After lowercase:  hey\n",
            "Tokens: ['hey']\n",
            "Filtered Tokens : ['hey']\n",
            "After removing punctuations: ['hey']\n",
            "After Lemmatization : ['hey']\n",
            "Final Tokens: ['hey']\n",
            "Cleaned Text: hey\n",
            "==================================================\n",
            "After lowercase:  good morning\n",
            "Tokens: ['good', 'morning']\n",
            "Filtered Tokens : ['good', 'morning']\n",
            "After removing punctuations: ['good', 'morning']\n",
            "After Lemmatization : ['good', 'morning']\n",
            "Final Tokens: ['good', 'morning']\n",
            "Cleaned Text: good morning\n",
            "==================================================\n",
            "After lowercase:  what's the weather today\n",
            "Tokens: ['what', \"'s\", 'the', 'weather', 'today']\n",
            "Filtered Tokens : [\"'s\", 'weather', 'today']\n",
            "After removing punctuations: [\"'s\", 'weather', 'today']\n",
            "After Lemmatization : [\"'s\", 'weather', 'today']\n",
            "Final Tokens: ['weather', 'today']\n",
            "Cleaned Text: weather today\n",
            "==================================================\n",
            "After lowercase:  what's the temperature\n",
            "Tokens: ['what', \"'s\", 'the', 'temperature']\n",
            "Filtered Tokens : [\"'s\", 'temperature']\n",
            "After removing punctuations: [\"'s\", 'temperature']\n",
            "After Lemmatization : [\"'s\", 'temperature']\n",
            "Final Tokens: ['temperature']\n",
            "Cleaned Text: temperature\n",
            "==================================================\n",
            "After lowercase:  is it raining\n",
            "Tokens: ['is', 'it', 'raining']\n",
            "Filtered Tokens : ['raining']\n",
            "After removing punctuations: ['raining']\n",
            "After Lemmatization : ['rain']\n",
            "Final Tokens: ['rain']\n",
            "Cleaned Text: rain\n",
            "==================================================\n",
            "After lowercase:  open google\n",
            "Tokens: ['open', 'google']\n",
            "Filtered Tokens : ['open', 'google']\n",
            "After removing punctuations: ['open', 'google']\n",
            "After Lemmatization : ['open', 'google']\n",
            "Final Tokens: ['open', 'google']\n",
            "Cleaned Text: open google\n",
            "==================================================\n",
            "After lowercase:  open facebook\n",
            "Tokens: ['open', 'facebook']\n",
            "Filtered Tokens : ['open', 'facebook']\n",
            "After removing punctuations: ['open', 'facebook']\n",
            "After Lemmatization : ['open', 'facebook']\n",
            "Final Tokens: ['open', 'facebook']\n",
            "Cleaned Text: open facebook\n",
            "==================================================\n",
            "After lowercase:  open youtube\n",
            "Tokens: ['open', 'youtube']\n",
            "Filtered Tokens : ['open', 'youtube']\n",
            "After removing punctuations: ['open', 'youtube']\n",
            "After Lemmatization : ['open', 'youtube']\n",
            "Final Tokens: ['open', 'youtube']\n",
            "Cleaned Text: open youtube\n",
            "==================================================\n",
            "After lowercase:  bye\n",
            "Tokens: ['bye']\n",
            "Filtered Tokens : ['bye']\n",
            "After removing punctuations: ['bye']\n",
            "After Lemmatization : ['bye']\n",
            "Final Tokens: ['bye']\n",
            "Cleaned Text: bye\n",
            "==================================================\n",
            "After lowercase:  googdbye\n",
            "Tokens: ['googdbye']\n",
            "Filtered Tokens : ['googdbye']\n",
            "After removing punctuations: ['googdbye']\n",
            "After Lemmatization : ['googdbye']\n",
            "Final Tokens: ['googdbye']\n",
            "Cleaned Text: googdbye\n",
            "==================================================\n",
            "After lowercase:  exit\n",
            "Tokens: ['exit']\n",
            "Filtered Tokens : ['exit']\n",
            "After removing punctuations: ['exit']\n",
            "After Lemmatization : ['exit']\n",
            "Final Tokens: ['exit']\n",
            "Cleaned Text: exit\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UtFkxf4k4FC",
        "outputId": "373e3347-5dc0-4195-bbd9-511a95eb9907"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello',\n",
              " 'hello',\n",
              " 'hey',\n",
              " 'good morning',\n",
              " 'weather today',\n",
              " 'temperature',\n",
              " 'rain',\n",
              " 'open google',\n",
              " 'open facebook',\n",
              " 'open youtube',\n",
              " 'bye',\n",
              " 'googdbye',\n",
              " 'exit']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VMpxzaJ3k2VZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}